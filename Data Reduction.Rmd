---
title: "Data Reduction"
author: "Shaun Stearns"
date: "3/11/2020"
output: html_document
---

# knn
```{r}
# Load and preprocess data
utilities.df <- read.csv("Utilities.csv")
row.names(utilities.df) <- utilities.df[,1]
utilities.df <- utilities.df[,-1]
# Normalize Distance
utilities.df.norm <- sapply(utilities.df, scale)
row.names(utilities.df.norm) <- row.names(utilities.df)
# Run K-Means algorithm
km <- kmeans(utilities.df.norm, 6)
# Show Cluster Membership
km$cluster

#####Table 15.10 Cluster Centroids and Squared Distances
# Centroid
km$centers
# Within-cluster sum of squares
km$withinss
# Cluster size
km$size

#####Figure 15.5 Code for plotting profile of centroids
#Plot an empty scatter plot
plot(c(0), xaxt = 'n', ylab = "", type = "l",
     ylim = c(min(km$centers), max(km$centers)), xlim = c(0,8))
#Label x-axes
axis(1, at = c(1:8), labels = names(utilities.df))
#Plot Centroids
for (i in c(1:6))
lines(km$centers[i,], lty = 1, lwd = 2, col = ifelse(i %in% c(1, 3, 5),
                                                     "black", "dark grey"))
#Name Clusters
text(x = 0.5, y = km$centers[,1], labels = paste("Cluster", c(1:6)))
dist(km$centers)
```

# Cluster
```{r}
#####Table 15.2 Distance Matrix between pairs
utilities.df <- read.csv("Utilities.csv")
#Set Row Names to the utilities column
row.names(utilities.df) <- utilities.df[,1]
#Remove the utility column
utilities.df <- utilities.df[,-1]
#Compute Euclidean Distance (to compute other distance measures, change the value in method =)
d <- dist(utilities.df[,c(6,8)], method = "euclidean")
d


#####Table 15.4 Normalized Distance Matrix between pairs
#Normalize input variables
utilities.df.norm <- sapply(utilities.df, scale)
#Add row names: utilities
row.names(utilities.df.norm) <- row.names(utilities.df)
#Compute Normalized Distance based on Sales (col 6) and Fuel Cost (Col 8)
d.norms <- dist(utilities.df.norm[,c(6,8)], method = "euclidean")
d.norms



#####Figure 15.3 Dendogram: Single Linkage and Average Linkage
d.norm <- dist(utilities.df.norm, method = "euclidean")
# in hclust() set argument 
# method = to "ward.D", "single", "complete", "average", "median", or "centroid"
hc1 <- hclust(d.norm, method = "single")
plot(hc1, hang = -1, ann = FALSE)
hc2 <- hclust(d.norm, method = "average")
plot(hc2, hang = -1, ann = FALSE)



#####Table 16.5 Computing Cluster Membership by cutting the dendogram
memb <- cutree(hc1, k = 6)
memb
memb <- cutree(hc2, k = 6)
memb



#####Figure 15.4 Heatmap fpr clusters
#Set labels as clustermembership and utility name
row.names(utilities.df.norm) <- paste(memb, ": ", row.names(utilities.df), sep = "")
#Plot Heatmap
#rev() reverses the color mapping to large = dark
heatmap(as.matrix(utilities.df.norm), Colv = NA, hclustfun = hclust,
        col=rev(paste("gray",1:99,sep = "")))



##### Table 15.9 K-Means clustering 22 utilities
#load and preprocess data
utilities.df <- read.csv("Utilities.csv")
row.names(utilities.df) <- utilities.df[,1]
utilities.df <- utilities.df[,-1]
#Normalize Distance
utilities.df.norm <- sapply(utilities.df, scale)
row.names(utilities.df.norm) <- row.names(utilities.df)
#Run K-Means algorithm
km <- kmeans(utilities.df.norm, 6)
#Show Cluster Membership
km$cluster



#####Table 15.10 Cluster Centroids and Squared Distances
#Centroid
km$centers
#Within-cluster sum of squares
km$withinss
#Cluster size
km$size



#####Figure 15.5 Code for plotting profile of centroids
#Plot an empty scatter plot
plot(c(0), xaxt = 'n', ylab = "", type = "l",
     ylim = c(min(km$centers), max(km$centers)), xlim = c(0,8))
#Label x-axes
axis(1, at = c(1:8), labels = names(utilities.df))
#Plot Centroids
for (i in c(1:6))
lines(km$centers[i,], lty = 1, lwd = 2, col = ifelse(i %in% c(1, 3, 5),
                                                     "black", "dark grey"))
#Name Clusters
text(x = 0.5, y = km$centers[,1], labels = paste("Cluster", c(1:6)))



#####Table 15.11
dist(km$centers)
```

# PCA (Rows)
```{r}
#### Table 4.3

boston.housing.df <- read.csv("BostonHousing.csv", header = TRUE) 
head(boston.housing.df, 9)
summary(boston.housing.df) 

# compute mean, standard dev., min, max, median, length, and missing values of CRIM
mean(boston.housing.df$CRIM) 
sd(boston.housing.df$CRIM)
min(boston.housing.df$CRIM)
max(boston.housing.df$CRIM)
median(boston.housing.df$CRIM) 
length(boston.housing.df$CRIM) 

# find the number of missing values of variable CRIM
sum(is.na(boston.housing.df$CRIM)) 

# compute mean, standard dev., min, max, median, length, and missing values for all
# variables
data.frame(mean=sapply(boston.housing.df, mean), 
                         sd=sapply(boston.housing.df, sd), 
                         min=sapply(boston.housing.df, min), 
                         max=sapply(boston.housing.df, max), 
                         median=sapply(boston.housing.df, median), 
                         length=sapply(boston.housing.df, length),
                         miss.val=sapply(boston.housing.df, function(x) 
                         sum(length(which(is.na(x))))))



#### Table 4.4

round(cor(boston.housing.df),2)



#### Table 4.5

table(boston.housing.df$CHAS)



#### Table 4.6

# create bins of size 1
boston.housing.df$RM.bin <- .bincode(boston.housing.df$RM, c(1:9))

# compute the average of MEDV by (binned) RM and CHAS
# in aggregate() use the argument by= to define the list of aggregating variables, 
# and FUN= as an aggregating function.
aggregate(boston.housing.df$MEDV, by=list(RM=boston.housing.df$RM.bin, 
                                          CHAS=boston.housing.df$CHAS), FUN=mean) 



#### Table 4.7

# use install.packages("reshape") the first time the package is used
library(reshape) 
boston.housing.df <- read.csv("BostonHousing.csv")
# create bins of size 1
boston.housing.df$RM.bin <- .bincode(boston.housing.df$RM, c(1:9)) 

# use melt() to stack a set of columns into a single column of data.
# stack MEDV values for each combination of (binned) RM and CHAS
mlt <- melt(boston.housing.df, id=c("RM.bin", "CHAS"), measure=c("MEDV"))
head(mlt, 5)

# use cast() to reshape data and generate pivot table
cast(mlt, RM.bin ~ CHAS, subset=variable=="MEDV", 
     margins=c("grand_row", "grand_col"), mean)




#### Figure 4.1

library(ggmap)

tbl <- table(boston.housing.df$CAT..MEDV, boston.housing.df$ZN)
prop.tbl <- prop.table(tbl, margin=2)
barplot(prop.tbl, xlab="ZN", ylab="", yaxt="n",main="Distribution of CAT.MEDV by ZN")
axis(2, at=(seq(0,1, 0.2)), paste(seq(0,100,20), "%"))



#### Table 4.10

cereals.df <- read.csv("Cereals.csv") 
# compute PCs on two dimensions
pcs <- prcomp(data.frame(cereals.df$calories, cereals.df$rating)) 
summary(pcs) 
pcs$rot
scores <- pcs$x
head(scores, 5)



#### Table 4.11

pcs <- prcomp(na.omit(cereals.df[,-c(1:3)])) 
summary(pcs)




#### Table 4.12

pcs.cor <- prcomp(na.omit(cereals.df[,-c(1:3)]), scale. = T)
summary(pcs.cor)
```

# EFA (Variables)
```{r}
library(psych)
library(psychTools)
library(GPArotation)
cereals.df <- read.csv(file.choose(),header=TRUE)
t(t(names(cereals.df)))
cereals.df <- cereals.df[-c(1,2,3,16)]

# Perform EDA
library(DataExplorer)
create_report(cereals.df)

#Impute Missing Values with Mean
library(imputeTS)
cereals.df <- na_mean(cereals.df)

# Normalize Values
cereals.df = scale(cereals.df, center = TRUE, scale = TRUE)
cereals.df <- as.data.frame(cereals.df)


library(DataExplorer)
create_report(cereals.df)

# 
parallel <- fa.parallel(cereals.df, fm = 'minres', fa = 'fa')
print(parallel)

#
threefactor <- fa(cereals.df,nfactors = 3,rotate = "varimax", fm="ols")
print(threefactor)
print(threefactor$loadings,cutoff = 0.3)
fa.diagram(threefactor)

#t
fourfactor <- fa(cereals.df, nfactors = 4,rotate = "varimax", fm="minres")
print(fourfactor$loadings,cutoff = 0.3)
fa.diagram(fourfactor)
```

# Heirarchical Clustering
```{r}
library(stats)
library(cluster)
iris <- datasets::iris
iris2 <- iris[,-5]
species_labels <- iris[,5]
library(colorspace) # get nice colors
species_col <- rev(rainbow_hcl(3))[as.numeric(species_labels)]


# Plot a SPLOM:
pairs(iris2, col = species_col,
      lower.panel = NULL,
       cex.labels=2, pch=19, cex = 1.2)

# Add a legend
par(xpd = TRUE)
legend(x = 0.05, y = 0.4, cex = 2,
   legend = as.character(levels(species_labels)),
    fill = unique(species_col))
par(xpd = NA)



# Create Heirarchical Model
d_iris <- dist(iris2) # method="man" # is a bit better
hc_iris <- hclust(d_iris, method = "complete")
iris_species <- rev(levels(iris[,5]))

library(dendextend)
dend <- as.dendrogram(hc_iris)
# order it the closest we can to the order of the observations:
dend <- rotate(dend, 1:150)

# Color the branches based on the clusters:
dend <- color_branches(dend, k=3) #, groupLabels=iris_species)

# Manually match the labels, as much as possible, to the real classification of the flowers:
labels_colors(dend) <-
   rainbow_hcl(3)[sort_levels_values(
      as.numeric(iris[,5])[order.dendrogram(dend)]
   )]

# Add the flower type to the labels:
labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                           "(",labels(dend),")", 
                           sep = "")
# We hang the dendrogram a bit:
dend <- hang.dendrogram(dend,hang_height=0.1)
# reduce the size of the labels:
dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
#dend <- set(dend, "labels_cex", 0.5)
# And plot:
par(mar = c(3,3,3,7))
plot(dend, 
     main = "Clustered Iris data set
     (the labels give the true flower species)", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = iris_species, fill = rainbow_hcl(3))

# Circular Treemap
library(circlize)
par(mar = rep(0,4))
circlize_dendrogram(dend)


# Differnce in Accuracy Between Cluster Methods
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", 
        "median", "centroid", "ward.D2")
iris_dendlist <- dendlist()
for(i in seq_along(hclust_methods)) {
   hc_iris <- hclust(d_iris, method = hclust_methods[i])   
   iris_dendlist <- dendlist(iris_dendlist, as.dendrogram(hc_iris))
}
names(iris_dendlist) <- hclust_methods
iris_dendlist

hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", 
        "median", "centroid", "ward.D2")
iris_dendlist <- dendlist()
for(i in seq_along(hclust_methods)) {
   hc_iris <- hclust(d_iris, method = hclust_methods[i])   
   iris_dendlist <- dendlist(iris_dendlist, as.dendrogram(hc_iris))
}
names(iris_dendlist) <- hclust_methods
iris_dendlist

iris_dendlist_cor <- cor.dendlist(iris_dendlist)
iris_dendlist_cor
# Pie Plot Matrix
corrplot::corrplot(iris_dendlist_cor, "pie", "lower")

# Comparison of Differnet Methods
get_ordered_3_clusters <- function(dend) {
   cutree(dend, k = 3)[order.dendrogram(dend)]
}

dend_3_clusters <- lapply(iris_dendlist, get_ordered_3_clusters)

compare_clusters_to_iris <- function(clus) {FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)}

clusters_performance <- sapply(dend_3_clusters, compare_clusters_to_iris)
dotchart(sort(clusters_performance), xlim = c(0.7,1),
         xlab = "Fowlkes-Mallows Index (from 0 to 1)",
         main = "Perormance of clustering algorithms \n in detecting the 3 species",
         pch = 19)

# Treemap of all methods

par(mfrow = c(4,2))
for(i in 1:8) {
   iris_dendlist[[i]] %>% set("branches_k_color", k=2) %>% plot(axes = FALSE, horiz = TRUE)
   title(names(iris_dendlist)[i])
}
```